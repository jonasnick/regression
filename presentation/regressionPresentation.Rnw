% regressionPresentationRnw
% R code style:
\documentclass{beamer}
\usetheme{Darmstadt}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{moresize}

\SweaveOpts{keep.source=TRUE}
\title{Lineare Regression}
\subtitle{Kleinste Quadrate, Maximum Likelihood und Bayes}
\author{Jonas Nick}
\beamertemplatenavigationsymbolsempty
\setbeamertemplate{footline}[frame number]
\setbeamertemplate{section in toc}{\inserttocsection}

\begin{document}
\DefineVerbatimEnvironment{Sinput}{Verbatim}{xleftmargin=2em}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{xleftmargin=2em}
\DefineVerbatimEnvironment{Scode}{Verbatim}{xleftmargin=2em}
\fvset{listparameters={\setlength{\topsep}{0pt}}}
\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}
%\DefineVerbatimEnvironment{Sinput}{Verbatim}{fontsize=\fontsize{7}{9},fontshape=n}
%\DefineVerbatimEnvironment{Soutput}{Verbatim}{fontsize=\fontsize{7}{9},fontshape=n}
%\DefineVerbatimEnvironment{Scode}{Verbatim}{fontsize=\fontsize{7}{9},fontshape=n}


%Shortens output lines
<<echo=false>>=
options(width=100)
options(continue=" ")
options(prompt=" ")
source("regressionFunctions.R")
@
\begin{frame}[plain]{}
\titlepage
<<loadProfitData, echo=false>>=
profitData <- read.table("../data/houseprices.txt", 
                            header=TRUE, sep=",")
@
<<displayHead, echo=false>>=
#head(profitData)
@
<<displayProfitData2, fig=true, echo=false, include=false,width=6,height=4>>=
#lines(density(profitData$price))
#head(profitData$price)
#hist(profitData$price, breaks=10,prob=T)
plot(price~house_size,data=profitData, xlab="Hausgröße",
        yaxt="n",ylab="Preis");
ymin=min(profitData[,3])
ymax=max(profitData[,3])
axis(2, at=c(ymin, (ymin+ymax)/2, ymax))
        
model<-lm(price~house_size,data=profitData)
abline(model)
#plot(1:10)
@
<<displayProfitData0, fig=true, echo=false, include=false,width=6,height=4>>=
plot(price~house_size,data=profitData, xlab="",ylab="",xaxt="n",yaxt="n")
        
model<-lm(price~house_size,data=profitData)
abline(model)

@
\vspace*{-1cm}
\begin{figure}[tbp]
\includegraphics[height=45mm, width=60mm]{regressionPresentation-displayProfitData0}
\end{figure}

\end{frame}

\begin{frame}[fragile]{Lineare Regression: Least-squares, Maximum Likelihood und Bayes}
\small\tableofcontents
\end{frame}
\section{Problembeschreibung}
\begin{frame}[fragile]{Problembeschreibung}
\begin{small}
\[
x = \begin{bmatrix} 1\\x_1\\x_2\\\vdots\\x_n \end{bmatrix} \in \mathbb{R}^{n},
\:\: y \in \mathbb{R} \land y \in G (Gruppen)
\]

\begin{block}{Regression}
\[
y = h(x) + \epsilon
\]
\end{block}
Praktische Fragestellung, X und Y aus dem Trainingsdatensatz, x und y aus dem Testdatensatz:
\[
\underset{y}{\operatorname{argmax}}\, p(y|x,X,Y) 
\]


\end{small}
\end{frame}
\section{Regressionsverfahren}
\subsection{Lineare Regression}
\begin{frame}{Lineare Regression}
\begin{small}
\begin{columns}[T]
\begin{column}{.5\textwidth}
\[
\theta = \begin{bmatrix} \theta_0\\\theta_1\\\theta_2\\\vdots\\\theta_n \end{bmatrix} \in \mathbb{R}^{n+1}
\]
\vspace*{5mm}
\[
h_\theta(x)=\theta^Tx
\]
\[
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\dots+\theta_nx_n
\]
\end{column}
\begin{column}{.5\textwidth}
\begin{figure}[tbp]
\centering
\includegraphics[height=45mm, width=60mm]{regressionPresentation-displayProfitData2}
\end{figure}
\end{column}
\end{columns}
\end{small}
\end{frame}

\subsection{Nichtlineare Regression}
<<plotNonLinearData, echo=false, fig=true, include=false>>=
#nonLinearData <- generateDataset(100, xmin=0, xmax=4)
    
#plot(Y~X,data=nonLinearData, xlab="x",
#        ylab="y");
n<-100
x<-seq(n)
y<-rnorm(n,50+ 30*x^(-0.2),1)
Data<-data.frame(x,y)
plot(y~x,Data)
model<-nls(y~a + c*x^(-d),data=Data, start = list(a=80, c=20, d=0.2))
#loess_fit <- loess(y~x,Data)
#model<-nls(Y~a + c*X^(-d),data=nonLinearData, start = list(a=80, c=20, d=0.2))
lines(Data$x, predict(model))
#abline(model)
@

\begin{frame}{Nichtlineare Regression}
\begin{small}
\begin{block}{Nichtlineare Regression}
Linearkombination der Basisfunktionen $\phi$
\begin{align*}
h_\theta(x)&=\theta_0 + \theta_1\phi_1(x) + \theta_2\phi_2(x) + \theta_3\phi_3(x)\\
\end{align*}
\end{block}
\begin{columns}[T]
\begin{column}{.5\textwidth}
Beispiel:
\begin{align*}
\phi_1(x) &= x_1,\\
\phi_2(x) &= 0.1x_2\\ 
\phi_3(x) &= 30x_3^{-0.2}
\end{align*}
\end{column}
\begin{column}{.5\textwidth}
\begin{figure}[tbp]
\centering
\includegraphics[height=40mm, width=40mm]{regressionPresentation-plotNonLinearData}
\end{figure}
\end{column}
\end{columns}
\end{small}
\end{frame}

\subsection{Logistische Regression}
\begin{frame}[fragile]{Logistische Regression}
<<loadAdmissionData,echo=FALSE>>=
admissionData <- read.table("../data/admissionData.txt",header=TRUE,sep=",")
admissionData$admitted <- as.factor(admissionData$admitted)
@
<<sigmoidFunction, fig=true, echo=false, include=false,width=4,height=4>>=
par(cex.lab=1.5)
par(cex.axis=1.5)
plot(g, xlim=c(-5,5), ylim=c(-0.1, 1.2), xlab="z", ylab="g(z)", yaxt="n",xaxt="n")
axeX=seq(0, 1.2, 0.5)
axis(2, at=axeX, labels=axeX)
axeY=c(0)
axis(1, at=axeY, labels=axeY)
abline(v=0)
@
\begin{columns}[T]
\begin{column}{.5\textwidth}
\vspace*{1cm}
\[
y\in\left\{{0,1}\right\}
\]
\[
h_\theta(x) = g(\theta^Tx)
\]
\[
g(z) = \frac{1}{1+e^{-z}}\]
\vspace{0.3mm}
Falls $h_\theta(x) \ge 0.5$ \\
\hspace{1cm}prognostiziere "y=1"\\
sonst\\
\hspace{1cm}prognostiziere "y=0"
\end{column}
\begin{column}{.5\textwidth}
\vspace*{-1cm}
\begin{figure}[tbp]
\centering
\includegraphics[width=0.7\textwidth]{regressionPresentation-sigmoidFunction}
\end{figure}
<<admissionModel, echo=false>>=
mdl <- glm(admitted~exam1+exam2,data=admissionData,
            family="binomial")
@
<<admissionDataGLM, fig=true, echo=false, include=false,width=3,height=3>>=
slope <- coef(mdl)[2]/(-coef(mdl)[3])
intercept <- coef(mdl)[1]/(-coef(mdl)[3])
library(lattice)
xyplot(exam1 ~ exam2, data=admissionData, groups=admitted, 
            panel=function(...) {
                panel.xyplot(...)
                panel.abline(intercept,slope)
                panel.grid(...)
                })
@
<<glmSummary,echo=false>>=
#summary(mdl)
@
\vspace*{-1cm}
\begin{figure}[tbp]
\centering
\includegraphics[width=0.7\textwidth]{regressionPresentation-admissionDataGLM}
\end{figure}

\end{column}
\end{columns}
\end{frame}

\section{Parameterschätzung}
\begin{frame}
\begin{small}
\[
X = \begin{bmatrix} -x^{(1)^{T}}-\\-x^{(2)^{T}}-\\\vdots\\-x^{(m)^{T}}- \end{bmatrix} \in \mathbb{R}^{mxn}
\:Y = \begin{bmatrix} y^{(1)}\\y^{(2)}\\\vdots\\y^{(m)} \end{bmatrix} \in \mathbb{R}^{m}
\]
\begin{block}{Kostenfunktion}
\[
\hat{\theta} = \underset{\theta}{\operatorname{argmin}}\, J(\theta)
\]
\end{block}
\end{small}
\end{frame}

\subsection{Maximum Likelihood}
\begin{frame}{Maximum Likelihood Methode}
\begin{small}
\begin{block}{Maximum-Likelihood-Methode}
Es bezeichne $\mathcal{D} = (d^{(1)}, d^{(2)} \dots d^{(m)})$ Realisierungen von Zufallsvariablen mit zugehöriger Wahrscheinlichkeitsdichte $p(\mathcal{D}|\delta)$.  
\[
L(\delta) := p(\mathcal{D}|\delta)
\]
Wähle zu den Beobachtungen $\mathcal{D}$ als Parameterschätzung denjenigen Parameter $\hat{\delta}$, für
den die Likelihood maximal ist, d.h.
\[
\hat{\delta} = \underset{\delta}{\operatorname{argmax}}\, L(\delta)
\]
\end{block}
Beispiel: 
\[
p(d|\mu,\sigma) = \mathcal{N}(\mu, \sigma) = 
    \frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(d-\mu)^2}{2\sigma^2})
\]
\end{small}
\end{frame}

\begin{frame}{Kleinste Quadrate Herleitung}
\begin{small}
Annahme für das Regressionsproblem: 
\[
L(\theta) = p(y|x, \theta) = \mathcal{N}(h_\theta(x), \sigma_1)
\]
\\
\begin{align*}
p(Y|X, \theta) &= \prod_{i=1}^{m}\mathcal{N}(h_\theta(x^{(i)}), \sigma_1) \\
\ln p(Y|X,\theta) &= -\frac{\sigma_1}{2}\sum_{n=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2 + \frac{m}{2}\ln\sigma_1 - \frac{m}{2}\ln(2\pi) 
\end{align*}
\end{small}
\end{frame}

\begin{frame}{Kleinste Quadrate (Least Squares)}
\begin{small}
\begin{block}{Kleinste Quadrate Kostenfunktion}
\[
J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})- y^{(i)})^2
\]
\end{block}

\begin{block}{Normalengleichung}
\begin{align*}
\frac{\partial}{\partial\theta_j}J(\theta) &= \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})
\,x_j^{(i)} \stackrel{!}{=} 0 \\
\Rightarrow \theta&=(X^TX)^{-1}X^Ty
\end{align*}
\end{block}
\end{small}
\end{frame}

<<plotCostFunction, fig=true, echo=false, include=false>>=
endValue <-function(start, step) {
    return(start+ 199*step)
}
theta1Start <- -60000
theta1Step <- 4000
theta1s <- seq(theta1Start, endValue(theta1Start, theta1Step/2), theta1Step/2)
theta2s <- seq(0, 199, 1)
costs <- matrix(rep(NA, 200*200), 200)
X <- data.matrix(data.frame(rep(1, length(profitData$house_size)),profitData$house_size))
Y <- profitData$price
for(i in 1:200) {
    for(j in 1:200) {
        costs[i, j] <- J(X, data.matrix(Y), c(theta1s[i], theta2s[j]))
    }
}
#costFunctionFrame = data.frame(theta1s = theta1s, theta2s = theta2s, costs = costs)
#wireframe(costs ~ theta1s * theta2s, costFunctionFrame)
#xyplot(costs ~ theta1s * theta2s, data=costFunctionFrame)
myLevels <-c(0,5000, 1000000,5000000, 10000000000)
costs <- matrix(log(c(costs), base=200), 200)
#contour(theta1s, theta2s, costs, nlevels=10)
par(cex.lab=1.5)
theta1 <- theta1s
theta2 <-theta2s
palette(gray(seq(0,1.0,len=10))) #gray scales; print old palette
image(theta1, theta2, costs, col=palette(), axes=FALSE)
contour(theta1, theta2, costs, add=TRUE, col="black")
#filled.contour(theta1s, theta2s, costs, nlevels=20, col=grey(seq(0,1,0.05)), xlab="theta1", ylab="theta2")
@
\subsection{Gradientenverfahren}
\begin{frame}{Gradientenverfahren}
\begin{small}
\begin{columns}[T]
\begin{column}{.5\textwidth}
\begin{block}{Gradientenverfahren}
$\alpha$: Lernrate \\
repeat until convergence \{\\
\hspace{5mm}    $\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)$\\
\}\\
\end{block}
\end{column}
\begin{column}{.5\textwidth}
\vspace*{-1cm}
\begin{figure}[tbp]
\centering
\includegraphics[height=40mm, width=40mm ]{regressionPresentation-plotCostFunction}
\end{figure}
\end{column}
\end{columns}
\end{small}
\end{frame}

\subsection{Regularisierung}
<<plotOverfitting, fig=true, echo=false, include=false,width=4,height=4>>=
plot(profitData$price~profitData$house_size,Data,xlab="",ylab="",xaxt="n",yaxt="n")
profitData <- profitData[with(profitData, order(house_size)), ]
model<-lm(price~house_size 
            + I(house_size^2)+ I(house_size^3) + I(house_size^4) + I(house_size^5) 
            + I(house_size^6) + I(house_size^7) + I(house_size^8) + I(house_size^9)
            ,data=profitData)
#loess_fit <- loess(y~x,Data)
#model<-nls(Y~a + c*X^(-d),data=nonLinearData, start = list(a=80, c=20, d=0.2))
lines(
    profitData$house_size, 
    predict(model), lwd=1)
#curve(sum(summary(model)$coefficients * c(1, x, x^2, x^3, x^4, x^5, x^6, x^7, x^8, x^9)), add=T) #5.097075e+07+-2.381713e+05*x+4.739686e+02*x^2,add=T)

@

\begin{frame}{Regularisierung}
\begin{small}
\vspace{-1.5mm}
\begin{center}
\footnotesize
Overfitting
\normalsize
\end{center}
\vspace{-1.5cm}
\begin{figure}[tbp]
\includegraphics[height=40mm, width=40mm ]{regressionPresentation-plotOverfitting}
\end{figure}
\vspace{-1cm}
\begin{block}{Regularisierte Kleinste Quadrate}
\[
J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2 + \lambda\sum_{i=1}^{n}\theta_j^2
\]
\end{block}
\end{small}
\end{frame}

\subsection{Logistische Regression}
\begin{frame}[fragile]{Logistische Regression}
\begin{small}
<<logFunction1, fig=true, echo=false, include=false,width=4,height=4>>=
par(cex.lab=1.5)
par(mfrow=c(1,1))
plot(log1, xlim=c(0,1), xlab="h(x)", ylab="-log(h(x)")
#plot(log2, xlim=c(0,1), xlab="h(x)", ylab="-log(1-h(x))")
@
<<logFunction2, fig=true, echo=false, include=false,width=4,height=4>>=
par(cex.lab=1.5)
par(mfrow=c(1,1))

plot(log2, xlim=c(0,1), xlab="h(x)", ylab="-log(1-h(x))")
@
\vspace*{-5mm}

\[
J(\theta) = \frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)}),y^{(i)})
\]
\begin{equation*}
Cost(h_\theta(x),y) = \begin{cases}
                        -\log(h_\theta(x))      & if\:y = 1\\
                        -\log(1-h_\theta(x))    & if\:y = 0 \\
                    \end{cases}
\end{equation*}

\vspace*{-10mm}
\begin{columns}[T]
\begin{column}{.5\textwidth}
\begin{figure}[tbp]
\includegraphics[height=40mm, width=40mm ]{regressionPresentation-logFunction1}
\end{figure}
\end{column}
\begin{column}{.5\textwidth}
\begin{figure}[tbp]
\includegraphics[height=40mm, width=40mm ]{regressionPresentation-logFunction2}
\end{figure}
\end{column}
\end{columns}
\vspace{-3mm}

\begin{block}{Logistische Regression Kostenfunktion}
\[
J(\theta) = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log h_\theta(x^{(i)})+ (1-y^{(i)})\log(1-h_\theta(x^{(i)}))
\]
\end{block}
\end{small}
\end{frame}

\section{Bayes'sche Lineare Regression}
\subsection{Bayes Inferenz}
\begin{frame}[fragile]{Bayes Inferenz}
\begin{small}
\begin{block}{Bayes-Inferenz}
Die Wahrscheinlichkeitsdichte von $\mathcal{D}$, gegeben $\delta$, sei $p(d|\delta)$
und $L(\delta) = p(\mathcal{D}|\delta)$ die Likelihoodfunktion. Für den unbekannten 
Parameter wird eine a priori Dichte 
\[
p(\delta)
\]
spezifiziert.
Dann ist die a posteriori Dichte über den Satz von Bayes bestimmt durch
\[
p(\delta|\mathcal{D}) = \frac{p(d|\delta)p(\delta)}{p(\mathcal{D})} = 
\frac{L(\delta)p(\delta)}{\int L(\delta)p(\delta)d\delta}
\]
\end{block}
\end{small}
\end{frame}

\subsection{Bayes-Schätzer}
\begin{frame}[fragile]{Bayes-Schätzer}
\begin{small}
\begin{block}{Maximum a posteriori (MAP) Schätzer}
Wähle denjenigen Parameterwert $\hat{\delta}_{MAP}$, für den die a posteriori Dichte maximal wird,
d.h.
\[
\hat{\delta}_{MAP} = \underset{\delta}{\operatorname{argmax}}\, L(\delta)p(\delta)
\]
\end{block}
\end{small}
\end{frame}

\subsection{Bayes'sche Lineare Regression}
\begin{frame}[fragile]{Bayes'sche Lineare Regression}
\begin{small}
Parametervektor $\theta$ \\
\hspace{1cm}a priori Wahrscheinlichkeitsdichte: $p(\theta) = \mathcal{N}(0, \sigma_0)$ \\
\hspace{1cm}Likelihood: \hspace{1cm}$L(\theta) = p(y|x, \theta) = \mathcal{N}(h_\theta(x), \sigma_1)$\\
\hspace{1cm}a posteriori Dichte: \hspace{1.3cm}$p(\theta|x,y) \propto p(y|x,\theta)p(\theta)$ \\
\vspace{0.5cm}
\begin{block}{Kostenfunktion}
\[
J(\theta) = \frac{\sigma_1}{2}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2 + \frac{\sigma_0}{2}\theta^T\theta
\]
\end{block}
\vspace{0.5cm}
\begin{block}{Vorhersage eines Zielmerkmals}
\[
p(y|x,X,Y) = \int p(y|x,\theta)p(\theta|X,Y)d\theta
\]
\end{block}
%coincides with maximum likelihood given a uniform prior distribution

\end{small}
\end{frame}

\section{Anwendung}
\subsection{Titanic}
\begin{frame}[fragile]{Datensatz}
<<loadTitanicData, echo=false>>=
titanicTrainData <- read.csv("../data/titanicTrain.csv")
titanicTrainData <- titanicTrainData[,!names(titanicTrainData) %in% c("name")]
titanicTrainData$pclass <- as.factor(titanicTrainData$pclass)
titanicTrainData$embarked <- as.factor(titanicTrainData$embarked)
titanicTrainData$sex <- as.factor(titanicTrainData$sex)
titanicTrainTest <- titanicTrainData
titanicTrainData$survived <- as.factor(titanicTrainData$survived)

titanicTrainTest <- na.omit(titanicTrainTest)
testModel <- function() {
  titanicTrainPrediction = sapply((predict(model, type="response", newdata=titanicTrainTest)),threshold)
  titanicTrainTest$survived <- as.numeric(titanicTrainTest$survived)
  difference = titanicTrainTest$survived - titanicTrainPrediction
  correctFraction = (1-sum(abs(difference))/length(difference))
  return(paste(c("Trainingsdaten: ",as.character(correctFraction*100),"% korrekte Vorhersagen"),collapse = ""))
}
@
\tiny
<<printTitanicData,echo=false>>=
head(titanicTrainData)
@

\vspace{-0.9mm}
\hspace{0.5cm}$\vdots$
\end{frame}

\begin{frame}{Datensatz}
<<plotTitanicAge, echo=false, include=false, fig=true>>=
par(cex.lab=1.8)
par(cex.axis=1.8)
plot(survived~age,data=titanicTrainData)
@
<<plotTitanicSex, echo=false, include=false, fig=true>>=
mosaic(survived~sex, data=titanicTrainData,shade = TRUE, 
       labeling_args=list(gp_labels=gpar(fontsize=20),
                          gp_varnames=gpar(fontsize=22)),
       legend_args=list(fontsize=16),
       margins=unit(4, "lines"),
       legend_width=unit(7, "lines"))
@
<<plotTitanicClass, echo=false, include=false, fig=true>>=
mosaic(survived~pclass, data=titanicTrainData, shade = TRUE, 
       labeling_args=list(gp_labels=gpar(fontsize=20),
                          gp_varnames=gpar(fontsize=22)),
       legend_args=list(fontsize=16),
       margins=unit(4, "lines"),
       legend_width=unit(7, "lines"))

@
\begin{figure}[tb]
\vspace{-0.8cm}
\includegraphics[height=50mm, width=50mm]{regressionPresentation-plotTitanicClass}
\hfill
\includegraphics[height=50mm, width=50mm]{regressionPresentation-plotTitanicSex}
\end{figure}
\vspace{-1.6cm}
\begin{figure}[b]
\includegraphics[height=45mm, width=45mm]{regressionPresentation-plotTitanicAge}
\end{figure}
\end{frame}

\begin{frame}[fragile]{Modell}
\begin{small}
{\small
<<fitFirstModel, echo=true>>=
model <- glm(survived~age + sex + I(pclass==1) 
                + I(pclass==2),data=titanicTrainData, 
                family=binomial("logit"))
@

<<analyseFirstModel,echo=false>>=
cat(testModel())
@
}
\end{small}
\end{frame}
\begin{frame}[fragile]{Modell}
\begin{small}

\vspace{-0.4cm}
{\fontsize{7}{9}
<<analyseFirstModel,echo=false>>=
summary(model)
@
}
\end{small}
\end{frame}

\begin{frame}[fragile]{Modell}
\begin{normalsize}
{\small
<<analyseLogAgeModel,echo=true>>=
model <- glm(survived~I(log(age)) + sex 
                + pclass +  sibsp,
                data=titanicTrainData, 
                family=binomial("logit"))
@
<<analyseLogAgeModel,echo=false>>=
cat(testModel())
cat("Testdaten: 75.1% korrekte Vorhersagen")
@
\hspace*{1.0cm}
}
\end{normalsize}
\end{frame}

\begin{frame}[fragile]{Modell}
\begin{small}

\vspace{-0.4cm}

{\fontsize{7}{9}
<<analyseLogAgeModel,echo=false>>=
summary(model)
@
}
\end{small}
\end{frame}

\section{Zusammenfassung}
\begin{frame}{Zusammenfassung}
\begin{itemize}
\item Ein Regressionsmodell verknüpft $y$ mit einer Funktion von $x$ und $\theta$.
\item Lineare Regression ist die Linearkombination von gewichteten erklärenden Merkmalen.
\item Kleinste Quadrate Kostenfunktion ergibt sich aus der Maximum Likelihood Parameterschätzung.
\item Logistische Regression klassifiziert mittels Verbindungsfunktion.
\item Bayes Regression nimmt eine a priori Verteilung der Koeffizienten an womit sich ihre a posteriori Verteilung berechnet.  

\end{itemize}
\begin{small}
\end{small}
\end{frame}


\end{document}
